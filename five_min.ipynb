{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib as mpl\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import datetime\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import tensorflow as tf\n",
                "import pathlib\n",
                "from tensorflow import keras\n",
                "from keras.layers import Dense,Reshape,Flatten,Input,Conv2D,Concatenate,Lambda,BatchNormalization,ReLU,Conv1D,DepthwiseConv1D\n",
                "from keras.callbacks import TensorBoard , ModelCheckpoint\n",
                "import sklearn  \n",
                "import pickle\n",
                "\n",
                "# import wandb\n",
                "# from wandb.keras import WandbCallback\n",
                "# wandb.init(project=\"five-min-project\", entity=\"slick-team\")\n",
                "\n",
                "mpl.rcParams['figure.figsize'] = (12, 8)\n",
                "mpl.rcParams['axes.grid'] = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#GETTING DATA\n",
                "\n",
                "df = pd.read_csv('data/sp500_with_indicators.csv')\n",
                "df.pop('Unnamed: 0')\n",
                "date_time = pd.to_datetime(df.pop('Date'))\n",
                "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
                "\n",
                "#Removing unnecessary columns\n",
                "\n",
                "CLOSE = df.pop('Close')\n",
                "OPEN = df.pop('Open') \n",
                "HIGH = df.pop('High') \n",
                "LOW = df.pop('Low') \n",
                "VOLUME = df.pop('Volume') \n",
                "SPREAD = df.pop('Spread')\n",
                "TICKVOL = df.pop('TickVol') \n",
                "BUY_OR_SELL = df.pop('Class')\n",
                "BUY_OR_SELL_NUMBER = df.pop('Class_Number')\n",
                "CANDLE_BODY = df.pop('Candle_Body')    \n",
                "CANDLE_UPPER_SHADOW = df.pop('Candle_Upper_Shadow')   \n",
                "CANDLE_LOWER_SHADOOW = df.pop('Candle_Lower_Shadow')   \n",
                "\n",
                "df['Candle_UPPER_SHADOW'] = CANDLE_UPPER_SHADOW\n",
                "df['Candle_BODY'] = CANDLE_BODY\n",
                "df['Candle_LOWER_SHADOOW'] = CANDLE_LOWER_SHADOOW\n",
                "\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#DATA PROCESSING CLASS\n",
                "\n",
                "class DataProcessing():\n",
                "\n",
                "    def __init__(self ,data:pd.core.frame.DataFrame , output:pd.core.series.Series , input_width:int, stockname:str,\n",
                "                    min_max :bool =False,minimum:float=1.0 , maximum:float=1.0):\n",
                "        \"\"\"init method of our DataProcessing class\n",
                "\n",
                "        Args:\n",
                "            data (pd.core.frame.DataFrame): all of our data\n",
                "            output (pd.core.series.Series): all of our outputs\n",
                "            input_width (int): window size\n",
                "            stockname (str): stock name \n",
                "            min_max (bool, optional):  wether we want min_max scaling.Defaults to False.\n",
                "            minimum (float, optional): mininum of scaled data. Defaults to 1.0.\n",
                "            maximum (float, optional): maximum of scaled data. Defaults to 1.0.\n",
                "        \"\"\"\n",
                "        self.stockname :str  = stockname \n",
                "        self.input_width : int = input_width\n",
                "        self.data :pd.core.frame.DataFrame = data\n",
                "        self.output : pd.core.series.Series = output\n",
                "        self.column_indices : list[str] = {name: i for i, name in enumerate(data.columns)}\n",
                "        self.num_features : int = data.shape[1]\n",
                "        \n",
                "        self.data_mean = self.data.mean()\n",
                "        self.data_std = self.data.std()\n",
                "        #slit into test and train data\n",
                "        # n = len(data)\n",
                "\n",
                "        # self.input_train_dataset = data[:int(0.90 * n)]\n",
                "        # self.output_train_dataset = output[:int(0.90 * n)]\n",
                "\n",
                "\n",
                "        # self.input_test_dataset = data[int(0.90 * n):]\n",
                "        # self.output_test_dataset = output[int(0.90 * n):]\n",
                "\n",
                "\n",
                "        #reset indecies\n",
                "        # self.input_test_dataset = self.input_test_dataset.reset_index()\n",
                "        # self.input_test_dataset.pop(\"index\")\n",
                "        \n",
                "        # self.output_test_dataset = self.output_test_dataset.reset_index()\n",
                "        # self.output_test_dataset = self.output_test_dataset[\"Class_Number\"]\n",
                "\n",
                "        #Normalizing The Data\n",
                "\n",
                "        # self.input_train_dataset, self.input_train_std ,self.input_train_mean = normalize(self.input_train_dataset)\n",
                "        # self.input_test_dataset ,_,_ = normalize(self.input_test_dataset,data_std=self.input_train_std,data_mean=self.input_train_mean)\n",
                "\n",
                "\n",
                "        # self.input_train_std = self.input_train_dataset.std()  \n",
                "        # self.input_train_mean = self.input_train_dataset.mean()\n",
                "        \n",
                "        # self.input_train_dataset = (self.input_train_dataset - self.input_train_mean) / self.input_train_std\n",
                "        # self.input_test_dataset = (self.input_test_dataset - self.input_train_mean) / self.input_train_std\n",
                "\n",
                "        #Min max scaling \n",
                "\n",
                "        # if(min_max):\n",
                "        #     self.input_train_dataset = min_max_scaler(self.input_train_dataset,minimum,maximum)\n",
                "        #     self.input_test_dataset = min_max_scaler(self.input_test_dataset,minimum,maximum)\n",
                "            \n",
                "\n",
                "    def plot_normalized_data(self):\n",
                "        data_std = (self.data - self.data_mean) / self.data_std\n",
                "        data_std = data_std.melt(var_name=\"Column\", value_name=\"Normalized\")\n",
                "        plt.figure(figsize=(40, 12))\n",
                "        ax = sns.violinplot(x=\"Column\", y=\"Normalized\", data=data_std)\n",
                "        _ = ax.set_xticklabels(self.data.keys(), rotation=90)\n",
                "\n",
                "    def make_windows(self,input_data:pd.core.frame.DataFrame , output_data:pd.core.series.Series , convert_to_numpy:bool = True):\n",
                "        \n",
                "        \"\"\"making windows\n",
                "\n",
                "        Args:\n",
                "            input_data (pd.core.frame.DataFrame): input data\n",
                "            output_data (pd.core.series.Series): output data\n",
                "            convert_to_numpy (bool, optional): whether we want to convert our windows to numpy or not. Defaults to True.\n",
                "\n",
                "        Returns:\n",
                "            2 lists/numpy arrays: input windows and output windows\n",
                "        \"\"\"\n",
                "        \n",
                "        window_input = []\n",
                "        window_output=[]\n",
                "\n",
                "        for i in range(self.input_width,len(input_data)):\n",
                "\n",
                "            window_input.append(input_data[i-self.input_width:i].reset_index())\n",
                "            \n",
                "            window_output.append(output_data[i])\n",
                "\n",
                "            window_input[-1].pop(\"index\")\n",
                "            \n",
                "            #convert pd.DataFrame to numpy\n",
                "            window_input[-1]= window_input[-1].to_numpy() \n",
                "\n",
                "        #convert list to numpy\n",
                "\n",
                "        if(convert_to_numpy):\n",
                "            window_input = np.asarray(window_input)\n",
                "            window_output = np.asarray(window_output)\n",
                "\n",
                "            window_output = tf.one_hot(window_output,depth=2)\n",
                "\n",
                "        return window_input,window_output  \n",
                "\n",
                "\n",
                "def normalize(data:pd.core.frame.DataFrame,data_std=None,data_mean=None):\n",
                "    if(data_std is None):\n",
                "        data_std = data.std()\n",
                "    if(data_mean is None):\n",
                "        data_mean = data.mean()\n",
                "\n",
                "\n",
                "    data_normalized = (data-data_mean)/data_std\n",
                "\n",
                "    return data_normalized, data_std,data_mean\n",
                "    \n",
                "def min_max_scaler(data:pd.core.frame.DataFrame,minimum:float,maximum:float):\n",
                "    data_max = data.describe().transpose()[\"max\"]\n",
                "    data_min = data.describe().transpose()[\"min\"]\n",
                "    data_std = (data-data_min)/(data_max-data_min)\n",
                "    data_scaled = data_std*(abs(minimum)+maximum)+minimum\n",
                "    return data_scaled\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = DataProcessing(  data=df,\n",
                "                        output=BUY_OR_SELL_NUMBER,\n",
                "                        input_width=256,\n",
                "                        stockname=\"sp500_with_Indicator\",\n",
                "                        min_max=True,\n",
                "                        minimum=-1.0,\n",
                "                        maximum=1.0\n",
                "                        )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaled_data ,_,_=normalize(df)\n",
                "\n",
                "scaled_df = min_max_scaler(scaled_data,-1.,1.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#MAKE, SAVE, LOAD\n",
                "def make():\n",
                "    input_window , output_window = data.make_windows(   \n",
                "                                                    input_data=scaled_df,\n",
                "                                                    output_data=BUY_OR_SELL_NUMBER,\n",
                "                                                    convert_to_numpy=False\n",
                "                                                )\n",
                "    return input_window,output_window\n",
                "\n",
                "def save(input_window,output_window):\n",
                "    with open(\"data/windows/input_window.pickle\",\"wb\") as fp:\n",
                "        pickle.dump(input_window,fp)\n",
                "\n",
                "    with open('data/windows/output_window.pickle','wb') as fp:\n",
                "        pickle.dump(output_window,fp)\n",
                "\n",
                "\n",
                "def load():\n",
                "    with open('data/windows/input_window.pickle','rb') as fp:\n",
                "        input_window=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/output_window.pickle','rb') as fp:\n",
                "        output_window=pickle.load(fp)\n",
                "    return input_window,output_window\n",
                "\n",
                "# input_window,output_window=make()\n",
                "# save(input_window,output_window)\n",
                "# input_window,output_window=load()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#MAKE, SAVE and LOAD train/test  \n",
                "def make_train_test(input_window,output_window):\n",
                "\n",
                "    test_input = []\n",
                "    test_output = []\n",
                "\n",
                "    test_input_24 = []\n",
                "    test_output_24 = []\n",
                "\n",
                "    random_indices = []\n",
                "\n",
                "    for j in range(200):\n",
                "\n",
                "        random_index=np.random.randint(0,len(input_window)-48)\n",
                "        random_indices.append(random_index)\n",
                "        inp = []\n",
                "        out = []\n",
                "\n",
                "        for i in range(48):\n",
                "            index=i+random_index\n",
                "            \n",
                "            inp.append(input_window.pop(index))\n",
                "            out.append(output_window.pop(index))\n",
                "\n",
                "        for i in range(24):\n",
                "            test_input_24.append(inp[i])\n",
                "            test_output_24.append(out[i])\n",
                "\n",
                "            test_input.append(inp[i+24])\n",
                "            test_output.append(out[i+24])\n",
                "\n",
                "\n",
                "    #CONVERT TO NUMPY\n",
                "    input_window = np.asarray(input_window)\n",
                "    output_window = np.asarray(output_window)\n",
                "    output_window = tf.one_hot(output_window,depth=2)\n",
                "\n",
                "    test_input = np.asarray(test_input)\n",
                "    test_output = np.asarray(test_output)\n",
                "    test_output = tf.one_hot(test_output,depth=2)\n",
                "\n",
                "    test_input_24 = np.asarray(test_input_24)\n",
                "    test_output_24 = np.asarray(test_output_24)\n",
                "    test_output_24 = tf.one_hot(test_output_24,depth=2)\n",
                "\n",
                "    return input_window,output_window,test_input,test_output,test_input_24,test_output_24,random_indices\n",
                "\n",
                "\n",
                "def save_train_test(input_window,output_window,test_input,test_output,test_input_24,test_output_24,random_indices):\n",
                "\n",
                "        with open('data/windows/train_data/input_window.pickle','wb') as fp:\n",
                "            pickle.dump(input_window,fp)\n",
                "\n",
                "        with open('data/windows/train_data/output_window.pickle','wb') as fp:\n",
                "            pickle.dump(output_window,fp)\n",
                "\n",
                "        with open('data/windows/test_data/test_input.pickle','wb') as fp:\n",
                "            pickle.dump(test_input,fp)\n",
                "\n",
                "        with open('data/windows/test_data/test_output.pickle','wb') as fp:\n",
                "            pickle.dump(test_output,fp)\n",
                "\n",
                "        with open('data/windows/test_data/test_input_24.pickle','wb') as fp:\n",
                "            pickle.dump(test_input_24,fp)\n",
                "\n",
                "        with open('data/windows/test_data/test_output_24.pickle','wb') as fp:\n",
                "            pickle.dump(test_output_24,fp)\n",
                "\n",
                "        with open('data/windows/test_data/random_indices.pickle','wb') as fp:\n",
                "            pickle.dump(random_indices,fp)\n",
                "\n",
                "\n",
                "def load_train_test():\n",
                "\n",
                "    with open('data/windows/train_data/input_window.pickle','rb') as fp:\n",
                "        input_window=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/train_data/output_window.pickle','rb') as fp:\n",
                "        output_window=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/test_data/test_input.pickle','rb') as fp:\n",
                "        test_input=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/test_data/test_output.pickle','rb') as fp:\n",
                "        test_output=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/test_data/test_input_24.pickle','rb') as fp:\n",
                "        test_input_24=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/test_data/test_output_24.pickle','rb') as fp:\n",
                "        test_output_24=pickle.load(fp)\n",
                "\n",
                "    with open('data/windows/test_data/random_indices.pickle','rb') as fp:\n",
                "        random_indices=pickle.load(fp)\n",
                "\n",
                "    return input_window,output_window,test_input,test_output,test_input_24,test_output_24,random_indices\n",
                "        \n",
                "# input_window,output_window,test_input,test_output,test_input_24,test_output_24,random_indices = make_train_test(input_window,output_window)\n",
                "# save_train_test(input_window,output_window,test_input,test_output,test_input_24,test_output_24,random_indices)\n",
                "input_window,output_window,test_input,test_output,test_input_24,test_output_24,random_indices = load_train_test()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compile_and_fit(model,modelname,data:DataProcessing,input_window,output_window ,modelcheckpoint:bool = False):\n",
                "    import os\n",
                "\n",
                "    MAX_EPOCHS = 10\n",
                "    # wandb.config = {\n",
                "    #         \"learning_rate\": 0.001,\n",
                "    #         \"epochs\": MAX_EPOCHS,\n",
                "    #         \"batch_size\": 64\n",
                "    #         }\n",
                "\n",
                "\n",
                "    #check if path is available\n",
                "    path = f'models/{modelname}/{data.stockname}/tensorboard/logs/fit'\n",
                "    pathlib.Path(path).mkdir(parents=True,exist_ok=True)\n",
                "\n",
                "    #tensorboard\n",
                "    log_dir =f'models/{modelname}/{data.stockname}/tensorboard/logs/fit/{datetime.datetime.now().strftime(\"(%Y-%m-%d)-(%H-%M-%S)\")}'\n",
                "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
                "\n",
                "    #modelcheckpoint\n",
                "    checkpoint_path = \"tmp/cp-{epoch:04d}.ckpt\"\n",
                "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
                "\n",
                "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
                "        filepath=checkpoint_path,\n",
                "        monitor='val_binary_accuracy',\n",
                "        verbose=1,\n",
                "        save_weights_only=True,\n",
                "        save_best_only=False\n",
                "    )\n",
                "\n",
                "    model.save_weights(checkpoint_path.format(epoch=0))\n",
                "\n",
                "\n",
                "    model.compile(loss='binary_crossentropy',\n",
                "                    optimizer='adam',\n",
                "                    metrics=[tf.metrics.BinaryAccuracy()])\n",
                "\n",
                "    if modelcheckpoint:\n",
                "        history = model.fit(    x=input_window,\n",
                "                                y= output_window,\n",
                "                                validation_data=(test_input,test_output),\n",
                "                                batch_size=64,\n",
                "                                shuffle=True,\n",
                "                                epochs=MAX_EPOCHS,\n",
                "                                verbose=2,\n",
                "                                callbacks=[tensorboard_callback,model_checkpoint_callback],\n",
                "                            )\n",
                "\n",
                "    else:\n",
                "        \n",
                "        history = model.fit(    \n",
                "                        x=input_window,\n",
                "                        y= output_window,\n",
                "                        validation_data=(test_input,test_output),\n",
                "                        batch_size=64,\n",
                "                        shuffle=True,\n",
                "                        epochs=MAX_EPOCHS,\n",
                "                        verbose=2,\n",
                "                        callbacks=[tensorboard_callback],\n",
                "                    )\n",
                "    \n",
                "    return history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_candle_to_1d_array(candles_reshape,filters):\n",
                "\n",
                "    conv2d_1 = Conv2D(filters=filters/1,kernel_size=(  4,3),strides=(2,1),name=\"conv2d_1\")(candles_reshape)\n",
                "    conv2d_2 = Conv2D(filters=filters/2,kernel_size=(  8,3),strides=(2,1),name=\"conv2d_2\")(candles_reshape)\n",
                "    conv2d_3 = Conv2D(filters=filters/3,kernel_size=( 16,3),strides=(2,1),name=\"conv2d_3\")(candles_reshape)\n",
                "    conv2d_4 = Conv2D(filters=filters/4,kernel_size=( 32,3),strides=(2,1),name=\"conv2d_4\")(candles_reshape)\n",
                "    conv2d_5 = Conv2D(filters=filters/5,kernel_size=( 64,3),strides=(2,1),name=\"conv2d_5\")(candles_reshape)\n",
                "    conv2d_6 = Conv2D(filters=filters/6,kernel_size=(128,3),strides=(2,1),name=\"conv2d_6\")(candles_reshape)\n",
                "\n",
                "    batch_norm_1 = BatchNormalization(name=\"batch_norm_1\")(conv2d_1)\n",
                "    batch_norm_2 = BatchNormalization(name=\"batch_norm_2\")(conv2d_2)\n",
                "    batch_norm_3 = BatchNormalization(name=\"batch_norm_3\")(conv2d_3)\n",
                "    batch_norm_4 = BatchNormalization(name=\"batch_norm_4\")(conv2d_4)\n",
                "    batch_norm_5 = BatchNormalization(name=\"batch_norm_5\")(conv2d_5)\n",
                "    batch_norm_6 = BatchNormalization(name=\"batch_norm_6\")(conv2d_6)\n",
                "\n",
                "    relu_1 = ReLU(name = \"relu_1\")(batch_norm_1)\n",
                "    relu_2 = ReLU(name = \"relu_2\")(batch_norm_2)\n",
                "    relu_3 = ReLU(name = \"relu_3\")(batch_norm_3)\n",
                "    relu_4 = ReLU(name = \"relu_4\")(batch_norm_4)\n",
                "    relu_5 = ReLU(name = \"relu_5\")(batch_norm_5)\n",
                "    relu_6 = ReLU(name = \"relu_6\")(batch_norm_6)\n",
                "\n",
                "    reshape_1 = Reshape(target_shape=(relu_1.shape[1],relu_1.shape[3]),name='reshape_1')(relu_1)\n",
                "    reshape_2 = Reshape(target_shape=(relu_2.shape[1],relu_2.shape[3]),name='reshape_2')(relu_2)\n",
                "    reshape_3 = Reshape(target_shape=(relu_3.shape[1],relu_3.shape[3]),name='reshape_3')(relu_3)\n",
                "    reshape_4 = Reshape(target_shape=(relu_4.shape[1],relu_4.shape[3]),name='reshape_4')(relu_4)\n",
                "    reshape_5 = Reshape(target_shape=(relu_5.shape[1],relu_5.shape[3]),name='reshape_5')(relu_5)\n",
                "    reshape_6 = Reshape(target_shape=(relu_6.shape[1],relu_6.shape[3]),name='reshape_6')(relu_6)\n",
                "\n",
                "    return reshape_1,reshape_2,reshape_3,reshape_4,reshape_5,reshape_6\n",
                "\n",
                "def one_stride_conv_block(reshape_1,reshape_2,reshape_3,reshape_4,reshape_5,reshape_6,filters , blockid:int):\n",
                "\n",
                "    depthwiseconv1d_1 = DepthwiseConv1D(kernel_size=  4 ,padding='same',name=f'depthconv1d_1_block{blockid}')(reshape_1)\n",
                "    depthwiseconv1d_2 = DepthwiseConv1D(kernel_size=  8 ,padding='same',name=f'depthconv1d_2_block{blockid}')(reshape_2)\n",
                "    depthwiseconv1d_3 = DepthwiseConv1D(kernel_size= 16 ,padding='same',name=f'depthconv1d_3_block{blockid}')(reshape_3)\n",
                "    depthwiseconv1d_4 = DepthwiseConv1D(kernel_size= 32 ,padding='same',name=f'depthconv1d_4_block{blockid}')(reshape_4)\n",
                "    depthwiseconv1d_5 = DepthwiseConv1D(kernel_size= 64 ,padding='same',name=f'depthconv1d_5_block{blockid}')(reshape_5)\n",
                "    depthwiseconv1d_6 = DepthwiseConv1D(kernel_size=128 ,padding='same',name=f'depthconv1d_6_block{blockid}')(reshape_6)\n",
                "\n",
                "    batch_norm_1 = BatchNormalization(name=f'batch_norm_1_1_block{blockid}')(depthwiseconv1d_1)\n",
                "    batch_norm_2 = BatchNormalization(name=f'batch_norm_1_2_block{blockid}')(depthwiseconv1d_2)\n",
                "    batch_norm_3 = BatchNormalization(name=f'batch_norm_1_3_block{blockid}')(depthwiseconv1d_3)\n",
                "    batch_norm_4 = BatchNormalization(name=f'batch_norm_1_4_block{blockid}')(depthwiseconv1d_4)\n",
                "    batch_norm_5 = BatchNormalization(name=f'batch_norm_1_5_block{blockid}')(depthwiseconv1d_5)\n",
                "    batch_norm_6 = BatchNormalization(name=f'batch_norm_1_6_block{blockid}')(depthwiseconv1d_6)\n",
                "\n",
                "    relu_1 = ReLU(name=f'relu_1_1_block{blockid}')(batch_norm_1)\n",
                "    relu_2 = ReLU(name=f'relu_1_2_block{blockid}')(batch_norm_2)\n",
                "    relu_3 = ReLU(name=f'relu_1_3_block{blockid}')(batch_norm_3)\n",
                "    relu_4 = ReLU(name=f'relu_1_4_block{blockid}')(batch_norm_4)\n",
                "    relu_5 = ReLU(name=f'relu_1_5_block{blockid}')(batch_norm_5)\n",
                "    relu_6 = ReLU(name=f'relu_1_6_block{blockid}')(batch_norm_6)\n",
                "\n",
                "    conv1d_1 = Conv1D(filters=filters/1,kernel_size=  4 ,padding='same',name=f'conv1d_1_block{blockid}')(relu_1)\n",
                "    conv1d_2 = Conv1D(filters=filters/2,kernel_size=  8 ,padding='same',name=f'conv1d_2_block{blockid}')(relu_2)\n",
                "    conv1d_3 = Conv1D(filters=filters/3,kernel_size= 16 ,padding='same',name=f'conv1d_3_block{blockid}')(relu_3)\n",
                "    conv1d_4 = Conv1D(filters=filters/4,kernel_size= 32 ,padding='same',name=f'conv1d_4_block{blockid}')(relu_4)\n",
                "    conv1d_5 = Conv1D(filters=filters/5,kernel_size= 64 ,padding='same',name=f'conv1d_5_block{blockid}')(relu_5)\n",
                "    conv1d_6 = Conv1D(filters=filters/6,kernel_size=128 ,padding='same',name=f'conv1d_6_block{blockid}')(relu_6)\n",
                "\n",
                "    batch_norm_1 = BatchNormalization(name=f'batch_norm_2_1_block{blockid}')(conv1d_1)\n",
                "    batch_norm_2 = BatchNormalization(name=f'batch_norm_2_2_block{blockid}')(conv1d_2)\n",
                "    batch_norm_3 = BatchNormalization(name=f'batch_norm_2_3_block{blockid}')(conv1d_3)\n",
                "    batch_norm_4 = BatchNormalization(name=f'batch_norm_2_4_block{blockid}')(conv1d_4)\n",
                "    batch_norm_5 = BatchNormalization(name=f'batch_norm_2_5_block{blockid}')(conv1d_5)\n",
                "    batch_norm_6 = BatchNormalization(name=f'batch_norm_2_6_block{blockid}')(conv1d_6)\n",
                "\n",
                "    relu_1 = ReLU(name=f'relu_2_1_block{blockid}')(batch_norm_1)\n",
                "    relu_2 = ReLU(name=f'relu_2_2_block{blockid}')(batch_norm_2)\n",
                "    relu_3 = ReLU(name=f'relu_2_3_block{blockid}')(batch_norm_3)\n",
                "    relu_4 = ReLU(name=f'relu_2_4_block{blockid}')(batch_norm_4)\n",
                "    relu_5 = ReLU(name=f'relu_2_5_block{blockid}')(batch_norm_5)\n",
                "    relu_6 = ReLU(name=f'relu_2_6_block{blockid}')(batch_norm_6)\n",
                "\n",
                "    return relu_1,relu_2,relu_3,relu_4,relu_5,relu_6\n",
                "\n",
                "def two_stride_conv_block(relu_1,relu_2,relu_3,relu_4,relu_5,relu_6,filters:int,blockid:int):\n",
                "\n",
                "    depthwiseconv1d_1 = DepthwiseConv1D(kernel_size=  4 ,padding='same',strides= 2,name=f'depthconv1d_1_block{blockid}')(relu_1)\n",
                "    depthwiseconv1d_2 = DepthwiseConv1D(kernel_size=  8 ,padding='same',strides= 2,name=f'depthconv1d_2_block{blockid}')(relu_2)\n",
                "    depthwiseconv1d_3 = DepthwiseConv1D(kernel_size= 16 ,padding='same',strides= 2,name=f'depthconv1d_3_block{blockid}')(relu_3)\n",
                "    depthwiseconv1d_4 = DepthwiseConv1D(kernel_size= 32 ,padding='same',strides= 2,name=f'depthconv1d_4_block{blockid}')(relu_4)\n",
                "    depthwiseconv1d_5 = DepthwiseConv1D(kernel_size= 64 ,padding='same',strides= 2,name=f'depthconv1d_5_block{blockid}')(relu_5)\n",
                "    depthwiseconv1d_6 = DepthwiseConv1D(kernel_size=128 ,padding='same',strides= 2,name=f'depthconv1d_6_block{blockid}')(relu_6)\n",
                "\n",
                "    batch_norm_1 = BatchNormalization(name=f'batch_norm_1_1_block{blockid}')(depthwiseconv1d_1)\n",
                "    batch_norm_2 = BatchNormalization(name=f'batch_norm_1_2_block{blockid}')(depthwiseconv1d_2)\n",
                "    batch_norm_3 = BatchNormalization(name=f'batch_norm_1_3_block{blockid}')(depthwiseconv1d_3)\n",
                "    batch_norm_4 = BatchNormalization(name=f'batch_norm_1_4_block{blockid}')(depthwiseconv1d_4)\n",
                "    batch_norm_5 = BatchNormalization(name=f'batch_norm_1_5_block{blockid}')(depthwiseconv1d_5)\n",
                "    batch_norm_6 = BatchNormalization(name=f'batch_norm_1_6_block{blockid}')(depthwiseconv1d_6)\n",
                "\n",
                "    relu_1 = ReLU(name=f'relu_1_1_block{blockid}')(batch_norm_1)\n",
                "    relu_2 = ReLU(name=f'relu_1_2_block{blockid}')(batch_norm_2)\n",
                "    relu_3 = ReLU(name=f'relu_1_3_block{blockid}')(batch_norm_3)\n",
                "    relu_4 = ReLU(name=f'relu_1_4_block{blockid}')(batch_norm_4)\n",
                "    relu_5 = ReLU(name=f'relu_1_5_block{blockid}')(batch_norm_5)\n",
                "    relu_6 = ReLU(name=f'relu_1_6_block{blockid}')(batch_norm_6)\n",
                "\n",
                "    conv1d_1 = Conv1D(filters=filters/1,kernel_size=  4 ,padding='same',name=f'conv1d_1_block{blockid}')(relu_1)\n",
                "    conv1d_2 = Conv1D(filters=filters/2,kernel_size=  8 ,padding='same',name=f'conv1d_2_block{blockid}')(relu_2)\n",
                "    conv1d_3 = Conv1D(filters=filters/3,kernel_size= 16 ,padding='same',name=f'conv1d_3_block{blockid}')(relu_3)\n",
                "    conv1d_4 = Conv1D(filters=filters/4,kernel_size= 32 ,padding='same',name=f'conv1d_4_block{blockid}')(relu_4)\n",
                "    conv1d_5 = Conv1D(filters=filters/5,kernel_size= 64 ,padding='same',name=f'conv1d_5_block{blockid}')(relu_5)\n",
                "    conv1d_6 = Conv1D(filters=filters/6,kernel_size=128 ,padding='same',name=f'conv1d_6_block{blockid}')(relu_6)\n",
                "\n",
                "    batch_norm_1 = BatchNormalization(name=f'batch_norm_2_1_block{blockid}')(conv1d_1)\n",
                "    batch_norm_2 = BatchNormalization(name=f'batch_norm_2_2_block{blockid}')(conv1d_2)\n",
                "    batch_norm_3 = BatchNormalization(name=f'batch_norm_2_3_block{blockid}')(conv1d_3)\n",
                "    batch_norm_4 = BatchNormalization(name=f'batch_norm_2_4_block{blockid}')(conv1d_4)\n",
                "    batch_norm_5 = BatchNormalization(name=f'batch_norm_2_5_block{blockid}')(conv1d_5)\n",
                "    batch_norm_6 = BatchNormalization(name=f'batch_norm_2_6_block{blockid}')(conv1d_6)\n",
                "\n",
                "    relu_1 = ReLU(name=f'relu_2_1_block{blockid}')(batch_norm_1)\n",
                "    relu_2 = ReLU(name=f'relu_2_2_block{blockid}')(batch_norm_2)\n",
                "    relu_3 = ReLU(name=f'relu_2_3_block{blockid}')(batch_norm_3)\n",
                "    relu_4 = ReLU(name=f'relu_2_4_block{blockid}')(batch_norm_4)\n",
                "    relu_5 = ReLU(name=f'relu_2_5_block{blockid}')(batch_norm_5)\n",
                "    relu_6 = ReLU(name=f'relu_2_6_block{blockid}')(batch_norm_6)\n",
                "\n",
                "    return relu_1,relu_2,relu_3,relu_4,relu_5,relu_6\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#candle MODEL\n",
                "\n",
                "input_layer = Input(shape=(256,3))\n",
                "reshaped_candles = Reshape(target_shape=(256,3,1),name=\"reshape_0\")(input_layer)\n",
                "#Converting candles into 1d arrays\n",
                "\n",
                "reshape_1,reshape_2,reshape_3,reshape_4,reshape_5,reshape_6 = convert_candle_to_1d_array(\n",
                "                                                                                            candles_reshape=reshaped_candles,\n",
                "                                                                                            filters=32\n",
                "                                                                                        )\n",
                "\n",
                "#Block1 with strides = 1, shape sizes of: 127 , 125 , 121 , 113 , 97 , 65 \n",
                "\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = one_stride_conv_block(\n",
                "                                                                    reshape_1,\n",
                "                                                                    reshape_2,\n",
                "                                                                    reshape_3,\n",
                "                                                                    reshape_4,\n",
                "                                                                    reshape_5,\n",
                "                                                                    reshape_6,\n",
                "                                                                    filters=32,\n",
                "                                                                    blockid=1\n",
                "                                                                )\n",
                "\n",
                "\n",
                "#Block 2 with stride = 2, shape sizes of: 64 , 63 , 61 , 57 , 49 , 33\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=64,\n",
                "                                                                    blockid=2\n",
                "                                                                )\n",
                "\n",
                "#Block 3 with stride = 2, shape sizes of: 32 , 32, 31, 29, 25 , 17\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=128,\n",
                "                                                                    blockid=3\n",
                "                                                                )\n",
                "\n",
                "#Block 4 with stride = 2, shape sizes of: 16, 16, 16, 15, 13,  9\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=256,\n",
                "                                                                    blockid=4\n",
                "                                                                )\n",
                "\n",
                "#Block 5 with stride = 2, shape sizes of: 8, 8, 8, 8, 7, 5\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=256,\n",
                "                                                                    blockid=5\n",
                "                                                                )\n",
                "\n",
                "\n",
                "global_avg_pool_1 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_1\")(relu_1)\n",
                "global_avg_pool_2 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_2\")(relu_2)\n",
                "global_avg_pool_3 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_3\")(relu_3)\n",
                "global_avg_pool_4 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_4\")(relu_4)\n",
                "global_avg_pool_5 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_5\")(relu_5)\n",
                "global_avg_pool_6 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_6\")(relu_6)\n",
                "\n",
                "\n",
                "candle_model = keras.Model(\n",
                "                        inputs = input_layer,\n",
                "\n",
                "                        outputs = [\n",
                "                                    global_avg_pool_1,\n",
                "                                    global_avg_pool_2,\n",
                "                                    global_avg_pool_3,\n",
                "                                    global_avg_pool_4,\n",
                "                                    global_avg_pool_5,\n",
                "                                    global_avg_pool_6\n",
                "                                ]\n",
                "                    )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#INDICATOR MODEL\n",
                "\n",
                "input_layer = Input(shape=(256,10))\n",
                "\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = one_stride_conv_block(\n",
                "                                                                    input_layer,\n",
                "                                                                    input_layer,\n",
                "                                                                    input_layer,\n",
                "                                                                    input_layer,\n",
                "                                                                    input_layer,\n",
                "                                                                    input_layer,\n",
                "                                                                    filters=32,\n",
                "                                                                    blockid=10\n",
                "                                                                )\n",
                "\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = one_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=32,\n",
                "                                                                    blockid=11\n",
                "                                                                )\n",
                "\n",
                "\n",
                "#Block 2 with stride = 2, shape sizes of: 64 , 63 , 61 , 57 , 49 , 33\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=64,\n",
                "                                                                    blockid=12\n",
                "                                                                )\n",
                "\n",
                "#Block 3 with stride = 2, shape sizes of: 32 , 32, 31, 29, 25 , 17\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=128,\n",
                "                                                                    blockid=13\n",
                "                                                                )\n",
                "\n",
                "#Block 4 with stride = 2, shape sizes of: 16, 16, 16, 15, 13,  9\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=256,\n",
                "                                                                    blockid=14\n",
                "                                                                )\n",
                "\n",
                "#Block 5 with stride = 2, shape sizes of: 8, 8, 8, 8, 7, 5\n",
                "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
                "                                                                    relu_1,\n",
                "                                                                    relu_2,\n",
                "                                                                    relu_3,\n",
                "                                                                    relu_4,\n",
                "                                                                    relu_5,\n",
                "                                                                    relu_6,\n",
                "                                                                    filters=256,\n",
                "                                                                    blockid=15\n",
                "                                                                )\n",
                "\n",
                "\n",
                "global_avg_pool_1 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_1\")(relu_1)\n",
                "global_avg_pool_2 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_2\")(relu_2)\n",
                "global_avg_pool_3 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_3\")(relu_3)\n",
                "global_avg_pool_4 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_4\")(relu_4)\n",
                "global_avg_pool_5 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_5\")(relu_5)\n",
                "global_avg_pool_6 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_6\")(relu_6)\n",
                "\n",
                "\n",
                "indicator_model = keras.Model(\n",
                "                        inputs = input_layer,\n",
                "                        outputs = [\n",
                "                                    global_avg_pool_1,\n",
                "                                    global_avg_pool_2,\n",
                "                                    global_avg_pool_3,\n",
                "                                    global_avg_pool_4,\n",
                "                                    global_avg_pool_5,\n",
                "                                    global_avg_pool_6\n",
                "                                ]\n",
                "                    )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#MODEL\n",
                "input_layer = Input(shape=(256,13))\n",
                "candles = Lambda(lambda x: x[:,:,-3:])(input_layer)\n",
                "indicators = Lambda(lambda x: x[:,:,:-3])(input_layer)\n",
                "\n",
                "gap_candle_1,gap_candle_2,gap_candle_3,gap_candle_4,gap_candle_5,gap_candle_6=candle_model(candles)\n",
                "gap_indicator_1,gap_indicator_2,gap_indicator_3,gap_indicator_4,gap_indicator_5,gap_indicator_6 = indicator_model(indicators)\n",
                "\n",
                "\n",
                "concatinate = Concatenate()([   gap_candle_1,\n",
                "                                gap_candle_2,\n",
                "                                gap_candle_3,\n",
                "                                gap_candle_4,\n",
                "                                gap_candle_5,\n",
                "                                gap_candle_6,\n",
                "                                gap_indicator_1,\n",
                "                                gap_indicator_2,\n",
                "                                gap_indicator_3,\n",
                "                                gap_indicator_4,\n",
                "                                gap_indicator_5,\n",
                "                                gap_indicator_6]\n",
                ")\n",
                "\n",
                "dense01 = Dense(1024 , activation='relu')(concatinate)\n",
                "output = Dense(2 ,activation='softmax')(dense01)\n",
                "\n",
                "model = keras.Model(\n",
                "    inputs = input_layer,\n",
                "    outputs = output\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# indicator_model.summary()\n",
                "# tf.keras.utils.plot_model(model ,'model.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#name of the model \n",
                "name = \"6_(functional)_(22_04_02)\"\n",
                "\n",
                "# compile_and_fit(model=model , modelname=name,\n",
                "#                 data = data,input_window=input_window,\n",
                "#                 output_window=output_window,modelcheckpoint=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model.save(f'models/{name}/model.h5')\n",
                "\n",
                "# model = keras.models.load_model(f'models/{name}/model.h5')\n",
                "# model.load_weights(f\"models/{name}/modelcheckpoint/cp-0003.ckpt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "m = tf.keras.metrics.BinaryAccuracy()\n",
                "m.update_state(y_true = test_output,y_pred = predicted2)\n",
                "m.result().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# a1 = np.ones((3,3),dtype=np.int8) + 0\n",
                "# a2 = np.ones((3,3),dtype=np.int8) + 1\n",
                "# a3 = np.ones((3,3),dtype=np.int8) + 2\n",
                "# a4 = np.ones((3,3),dtype=np.int8) + 3\n",
                "\n",
                "# a11 = np.ones((3,3),dtype=np.int8) * 10\n",
                "# a12 = np.ones((3,3),dtype=np.int8) * 20\n",
                "# a13 = np.ones((3,3),dtype=np.int8) * 30\n",
                "# a14 = np.ones((3,3),dtype=np.int8) * 40\n",
                "\n",
                "# inputs = np.array([a1,a2,a3,a4])\n",
                "# outputs = np.array([a11,a12,a13,a14])\n",
                "# a , b = sklearn.utils.shuffle(inputs,outputs)\n",
                "# print(f'{[a123 for a123 in a]},{[b123 for b123 in b]}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#PLOTING DISTRIBUTION OF PREDICTIONS ON DIFFERENT EPOCHS\n",
                "\n",
                "\n",
                "predictions = []\n",
                "\n",
                "NUMBER_OF_EPOCHS=10\n",
                "model = keras.Model(\n",
                "                        inputs = input_layer,\n",
                "                        outputs = output\n",
                "                    )\n",
                "for i in range(NUMBER_OF_EPOCHS):\n",
                "\n",
                "    model.load_weights(f\"models/{name}/modelcheckpoint/cp-00{i+1:02d}.ckpt\")\n",
                "    prediction = model.predict(test_input)\n",
                "    predictions.append(prediction)\n",
                "\n",
                "#PLOT\n",
                "mpl.rcParams['axes.grid'] = False\n",
                "mpl.rcParams['figure.figsize'] = (22, 40)\n",
                "fig, axes=plt.subplots(10,2)\n",
                "\n",
                "for i,value in enumerate(predictions):\n",
                "    for j in range(2):\n",
                "        if(j==0):\n",
                "            axes[i,j].hist(value[:,1],bins=50, label=f'epoch{i+1:02d}')\n",
                "            axes[i,j].legend(loc='best')\n",
                "\n",
                "        else:\n",
                "            axes[i,j].hist(test_output[:,1].numpy(),bins=50, label=f'epoch{i+1:02d}')\n",
                "            axes[i,j].legend(loc='best')\n",
                "\n",
                "            \n"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "2b0900fbe6dcf68ba8657d6a73781eea6c8e04d861aa42c88ba789e96c4944de"
        },
        "kernelspec": {
            "display_name": "Python 3.9.7 ('tensor')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
