{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense,Reshape,Flatten,Input,Conv2D,Concatenate,Lambda,BatchNormalization,ReLU,Conv1D,DepthwiseConv1D\n",
    "from keras.callbacks import TensorBoard , ModelCheckpoint\n",
    "import sklearn  \n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# wandb.init(project=\"five-min-project\", entity=\"slick-team\")\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING DATA\n",
    "\n",
    "df = pd.read_csv('data/sp500_with_indicators.csv')\n",
    "df.pop('Unnamed: 0')\n",
    "date_time = pd.to_datetime(df.pop('Date'))\n",
    "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "#Removing unnecessary columns\n",
    "\n",
    "CLOSE = df.pop('Close')\n",
    "OPEN = df.pop('Open') \n",
    "HIGH = df.pop('High') \n",
    "LOW = df.pop('Low') \n",
    "VOLUME = df.pop('Volume') \n",
    "SPREAD = df.pop('Spread')\n",
    "TICKVOL = df.pop('TickVol') \n",
    "BUY_OR_SELL = df.pop('Class')\n",
    "BUY_OR_SELL_NUMBER = df.pop('Class_Number')\n",
    "CANDEL_BODY = df.pop('Candel_Body')    \n",
    "CANDEL_UPPER_SHADOW = df.pop('Candel_Upper_Shadow')   \n",
    "CANDEL_LOWER_SHADOOW = df.pop('Candel_Lower_Shadow')   \n",
    "\n",
    "df['CANDEL_UPPER_SHADOW'] = CANDEL_UPPER_SHADOW\n",
    "df['CANDEL_BODY'] = CANDEL_BODY\n",
    "df['CANDEL_LOWER_SHADOOW'] = CANDEL_LOWER_SHADOOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING CLASS\n",
    "\n",
    "class DataProcessing():\n",
    "\n",
    "    def __init__(self ,data:pd.core.frame.DataFrame , output:pd.core.series.Series , input_width:int, stockname:str,\n",
    "                    min_max :bool =False,minimum:float=1.0 , maximum:float=1.0):\n",
    "\n",
    "        self.stockname :str  = stockname \n",
    "        self.input_width : int = input_width\n",
    "        self.data :pd.core.frame.DataFrame = data\n",
    "        self.output : pd.core.series.Series = output\n",
    "        self.column_indices : list[str] = {name: i for i, name in enumerate(data.columns)}\n",
    "        self.num_features : int = data.shape[1]\n",
    "        \n",
    "        self.data_mean = self.data.mean()\n",
    "        self.data_std = self.data.std()\n",
    "        #slit into test and train data\n",
    "        # n = len(data)\n",
    "\n",
    "        # self.input_train_dataset = data[:int(0.90 * n)]\n",
    "        # self.output_train_dataset = output[:int(0.90 * n)]\n",
    "\n",
    "\n",
    "        # self.input_test_dataset = data[int(0.90 * n):]\n",
    "        # self.output_test_dataset = output[int(0.90 * n):]\n",
    "\n",
    "\n",
    "        #reset indecies\n",
    "        # self.input_test_dataset = self.input_test_dataset.reset_index()\n",
    "        # self.input_test_dataset.pop('index')\n",
    "        \n",
    "        # self.output_test_dataset = self.output_test_dataset.reset_index()\n",
    "        # self.output_test_dataset = self.output_test_dataset['Class_Number']\n",
    "\n",
    "        #Normalizing The Data\n",
    "\n",
    "        # self.input_train_dataset, self.input_train_std ,self.input_train_mean = normalize(self.input_train_dataset)\n",
    "        # self.input_test_dataset ,_,_ = normalize(self.input_test_dataset,data_std=self.input_train_std,data_mean=self.input_train_mean)\n",
    "\n",
    "\n",
    "        # self.input_train_std = self.input_train_dataset.std()  \n",
    "        # self.input_train_mean = self.input_train_dataset.mean()\n",
    "        \n",
    "        # self.input_train_dataset = (self.input_train_dataset - self.input_train_mean) / self.input_train_std\n",
    "        # self.input_test_dataset = (self.input_test_dataset - self.input_train_mean) / self.input_train_std\n",
    "\n",
    "        #Min max scaling \n",
    "\n",
    "        # if(min_max):\n",
    "        #     self.input_train_dataset = min_max_scaler(self.input_train_dataset,minimum,maximum)\n",
    "        #     self.input_test_dataset = min_max_scaler(self.input_test_dataset,minimum,maximum)\n",
    "            \n",
    "\n",
    "    def plot_normalized_data(self):\n",
    "        data_std = (self.data - self.data_mean) / self.data_std\n",
    "        data_std = data_std.melt(var_name='Column', value_name='Normalized')\n",
    "        plt.figure(figsize=(40, 12))\n",
    "        ax = sns.violinplot(x='Column', y='Normalized', data=data_std)\n",
    "        _ = ax.set_xticklabels(self.data.keys(), rotation=90)\n",
    "\n",
    "    def make_windows(self,input_data:pd.core.frame.DataFrame , output_data:pd.core.series.Series , convert_to_numpy:bool = True):\n",
    "        \n",
    "        window_input = []\n",
    "        window_output=[]\n",
    "\n",
    "        for i in range(self.input_width,len(input_data)):\n",
    "\n",
    "            window_input.append(input_data[i-self.input_width:i].reset_index())\n",
    "            \n",
    "            window_output.append(output_data[i])\n",
    "\n",
    "            window_input[-1].pop('index')\n",
    "            \n",
    "            #convert pd.DataFrame to numpy\n",
    "            window_input[-1]= window_input[-1].to_numpy() \n",
    "\n",
    "        #convert list to numpy\n",
    "\n",
    "        if(convert_to_numpy):\n",
    "            window_input = np.asarray(window_input)\n",
    "            window_output = np.asarray(window_output)\n",
    "\n",
    "            window_output = tf.one_hot(window_output,depth=2)\n",
    "\n",
    "        return window_input,window_output  \n",
    "\n",
    "\n",
    "def normalize(data:pd.core.frame.DataFrame,data_std=None,data_mean=None):\n",
    "    if(data_std is None):\n",
    "        data_std = data.std()\n",
    "    if(data_mean is None):\n",
    "        data_mean = data.mean()\n",
    "\n",
    "\n",
    "    data_normalized = (data-data_mean)/data_std\n",
    "\n",
    "    return data_normalized, data_std,data_mean\n",
    "    \n",
    "def min_max_scaler(data:pd.core.frame.DataFrame,minimum:float,maximum:float):\n",
    "    data_max = data.describe().transpose()['max']\n",
    "    data_min = data.describe().transpose()['min']\n",
    "    data_std = (data-data_min)/(data_max-data_min)\n",
    "    data_scaled = data_std*(abs(minimum)+maximum)+minimum\n",
    "    return data_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataProcessing(  data=df,\n",
    "                        output=BUY_OR_SELL_NUMBER,\n",
    "                        input_width=256,\n",
    "                        stockname='sp500_with_Indicator',\n",
    "                        min_max=True,\n",
    "                        minimum=-1.0,\n",
    "                        maximum=1.0\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data ,_,_=normalize(df)\n",
    "\n",
    "scaled_df = min_max_scaler(scaled_data,-1.,1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window , output_window = data.make_windows(   \n",
    "                                                    input_data=scaled_df,\n",
    "                                                    output_data=BUY_OR_SELL_NUMBER,\n",
    "                                                    convert_to_numpy=False\n",
    "                                                )\n",
    "\n",
    "# output_window = output_window.numpy()\n",
    "# input_window,output_window = sklearn.utils.shuffle(input_window,output_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = []\n",
    "test_output = []\n",
    "\n",
    "test_input_24 = []\n",
    "test_output_24 = []\n",
    "\n",
    "\n",
    "for j in range(200):\n",
    "\n",
    "    random_index=np.random.randint(0,len(input_window)-48)\n",
    "\n",
    "    inp = []\n",
    "    out = []\n",
    "\n",
    "    for i in range(48):\n",
    "\n",
    "        index=i+random_index\n",
    "        \n",
    "        inp.append(input_window.pop(index))\n",
    "        out.append(output_window.pop(index))\n",
    "\n",
    "    for i in range(24):\n",
    "        test_input_24.append(inp[i])\n",
    "        test_output_24.append(out[i])\n",
    "\n",
    "        test_input.append(inp[i+24])\n",
    "        test_output.append(out[i+24])\n",
    "\n",
    "\n",
    "#CONVERT TO NUMPY\n",
    "input_window = np.asarray(input_window)\n",
    "output_window = np.asarray(output_window)\n",
    "output_window = tf.one_hot(output_window,depth=2)\n",
    "\n",
    "test_input = np.asarray(test_input)\n",
    "test_output = np.asarray(test_output)\n",
    "test_output = tf.one_hot(test_output,depth=2)\n",
    "\n",
    "test_input_24 = np.asarray(test_input_24)\n",
    "test_output_24 = np.asarray(test_output_24)\n",
    "test_output_24 = tf.one_hot(test_output_24,depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{input_window.shape=}')\n",
    "# print(f'{output_window.shape=}')\n",
    "\n",
    "# print(f'{test_input.shape=}')\n",
    "# print(f'{test_output.shape=}')\n",
    "\n",
    "# print(f'{test_input_24.shape=}')\n",
    "# print(f'{test_output_24.shape=}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model,modelname,data:DataProcessing,input_window,output_window ,modelcheckpoint:bool = False):\n",
    "    import os\n",
    "\n",
    "    MAX_EPOCHS = 10\n",
    "    # wandb.config = {\n",
    "    #         \"learning_rate\": 0.001,\n",
    "    #         \"epochs\": MAX_EPOCHS,\n",
    "    #         \"batch_size\": 64\n",
    "    #         }\n",
    "\n",
    "\n",
    "    #check if path is available\n",
    "    path = f'models/{modelname}/{data.stockname}/tensorboard/logs/fit'\n",
    "    pathlib.Path(path).mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "    #tensorboard\n",
    "    log_dir =f'models/{modelname}/{data.stockname}/tensorboard/logs/fit/{datetime.datetime.now().strftime(\"(%Y-%m-%d)-(%H-%M-%S)\")}'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    #modelcheckpoint\n",
    "    checkpoint_path = \"tmp/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_binary_accuracy',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=False\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=[tf.metrics.BinaryAccuracy()])\n",
    "\n",
    "    if modelcheckpoint:\n",
    "        history = model.fit(    x=input_window,\n",
    "                                y= output_window,\n",
    "                                validation_data=(test_input,test_output),\n",
    "                                batch_size=64,\n",
    "                                shuffle=True,\n",
    "                                epochs=MAX_EPOCHS,\n",
    "                                verbose=2,\n",
    "                                callbacks=[tensorboard_callback,model_checkpoint_callback],\n",
    "                            )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        history = model.fit(    \n",
    "                        x=input_window,\n",
    "                        y= output_window,\n",
    "                        validation_data=(test_input,test_output),\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        epochs=MAX_EPOCHS,\n",
    "                        verbose=2,\n",
    "                        callbacks=[tensorboard_callback],\n",
    "                    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_candel_to_1d_array(candels_reshape,filters):\n",
    "\n",
    "    conv2d_1 = Conv2D(filters=filters/1,kernel_size=(  4,3),strides=(2,1),name=\"conv2d_1\")(candels_reshape)\n",
    "    conv2d_2 = Conv2D(filters=filters/2,kernel_size=(  8,3),strides=(2,1),name=\"conv2d_2\")(candels_reshape)\n",
    "    conv2d_3 = Conv2D(filters=filters/3,kernel_size=( 16,3),strides=(2,1),name=\"conv2d_3\")(candels_reshape)\n",
    "    conv2d_4 = Conv2D(filters=filters/4,kernel_size=( 32,3),strides=(2,1),name=\"conv2d_4\")(candels_reshape)\n",
    "    conv2d_5 = Conv2D(filters=filters/5,kernel_size=( 64,3),strides=(2,1),name=\"conv2d_5\")(candels_reshape)\n",
    "    conv2d_6 = Conv2D(filters=filters/6,kernel_size=(128,3),strides=(2,1),name=\"conv2d_6\")(candels_reshape)\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name=\"batch_norm_1\")(conv2d_1)\n",
    "    batch_norm_2 = BatchNormalization(name=\"batch_norm_2\")(conv2d_2)\n",
    "    batch_norm_3 = BatchNormalization(name=\"batch_norm_3\")(conv2d_3)\n",
    "    batch_norm_4 = BatchNormalization(name=\"batch_norm_4\")(conv2d_4)\n",
    "    batch_norm_5 = BatchNormalization(name=\"batch_norm_5\")(conv2d_5)\n",
    "    batch_norm_6 = BatchNormalization(name=\"batch_norm_6\")(conv2d_6)\n",
    "\n",
    "    relu_1 = ReLU(name = \"relu_1\")(batch_norm_1)\n",
    "    relu_2 = ReLU(name = \"relu_2\")(batch_norm_2)\n",
    "    relu_3 = ReLU(name = \"relu_3\")(batch_norm_3)\n",
    "    relu_4 = ReLU(name = \"relu_4\")(batch_norm_4)\n",
    "    relu_5 = ReLU(name = \"relu_5\")(batch_norm_5)\n",
    "    relu_6 = ReLU(name = \"relu_6\")(batch_norm_6)\n",
    "\n",
    "    reshape_1 = Reshape(target_shape=(relu_1.shape[1],relu_1.shape[3]),name='reshape_1')(relu_1)\n",
    "    reshape_2 = Reshape(target_shape=(relu_2.shape[1],relu_2.shape[3]),name='reshape_2')(relu_2)\n",
    "    reshape_3 = Reshape(target_shape=(relu_3.shape[1],relu_3.shape[3]),name='reshape_3')(relu_3)\n",
    "    reshape_4 = Reshape(target_shape=(relu_4.shape[1],relu_4.shape[3]),name='reshape_4')(relu_4)\n",
    "    reshape_5 = Reshape(target_shape=(relu_5.shape[1],relu_5.shape[3]),name='reshape_5')(relu_5)\n",
    "    reshape_6 = Reshape(target_shape=(relu_6.shape[1],relu_6.shape[3]),name='reshape_6')(relu_6)\n",
    "\n",
    "    return reshape_1,reshape_2,reshape_3,reshape_4,reshape_5,reshape_6\n",
    "\n",
    "def one_stride_conv_block(reshape_1,reshape_2,reshape_3,reshape_4,reshape_5,reshape_6,filters , blockid:int):\n",
    "\n",
    "    depthwiseconv1d_1 = DepthwiseConv1D(kernel_size=  4 ,padding='same',name=f'depthconv1d_1_block{blockid}')(reshape_1)\n",
    "    depthwiseconv1d_2 = DepthwiseConv1D(kernel_size=  8 ,padding='same',name=f'depthconv1d_2_block{blockid}')(reshape_2)\n",
    "    depthwiseconv1d_3 = DepthwiseConv1D(kernel_size= 16 ,padding='same',name=f'depthconv1d_3_block{blockid}')(reshape_3)\n",
    "    depthwiseconv1d_4 = DepthwiseConv1D(kernel_size= 32 ,padding='same',name=f'depthconv1d_4_block{blockid}')(reshape_4)\n",
    "    depthwiseconv1d_5 = DepthwiseConv1D(kernel_size= 64 ,padding='same',name=f'depthconv1d_5_block{blockid}')(reshape_5)\n",
    "    depthwiseconv1d_6 = DepthwiseConv1D(kernel_size=128 ,padding='same',name=f'depthconv1d_6_block{blockid}')(reshape_6)\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name=f'batch_norm_1_1_block{blockid}')(depthwiseconv1d_1)\n",
    "    batch_norm_2 = BatchNormalization(name=f'batch_norm_1_2_block{blockid}')(depthwiseconv1d_2)\n",
    "    batch_norm_3 = BatchNormalization(name=f'batch_norm_1_3_block{blockid}')(depthwiseconv1d_3)\n",
    "    batch_norm_4 = BatchNormalization(name=f'batch_norm_1_4_block{blockid}')(depthwiseconv1d_4)\n",
    "    batch_norm_5 = BatchNormalization(name=f'batch_norm_1_5_block{blockid}')(depthwiseconv1d_5)\n",
    "    batch_norm_6 = BatchNormalization(name=f'batch_norm_1_6_block{blockid}')(depthwiseconv1d_6)\n",
    "\n",
    "    relu_1 = ReLU(name=f'relu_1_1_block{blockid}')(batch_norm_1)\n",
    "    relu_2 = ReLU(name=f'relu_1_2_block{blockid}')(batch_norm_2)\n",
    "    relu_3 = ReLU(name=f'relu_1_3_block{blockid}')(batch_norm_3)\n",
    "    relu_4 = ReLU(name=f'relu_1_4_block{blockid}')(batch_norm_4)\n",
    "    relu_5 = ReLU(name=f'relu_1_5_block{blockid}')(batch_norm_5)\n",
    "    relu_6 = ReLU(name=f'relu_1_6_block{blockid}')(batch_norm_6)\n",
    "\n",
    "    conv1d_1 = Conv1D(filters=filters/1,kernel_size=  4 ,padding='same',name=f'conv1d_1_block{blockid}')(relu_1)\n",
    "    conv1d_2 = Conv1D(filters=filters/2,kernel_size=  8 ,padding='same',name=f'conv1d_2_block{blockid}')(relu_2)\n",
    "    conv1d_3 = Conv1D(filters=filters/3,kernel_size= 16 ,padding='same',name=f'conv1d_3_block{blockid}')(relu_3)\n",
    "    conv1d_4 = Conv1D(filters=filters/4,kernel_size= 32 ,padding='same',name=f'conv1d_4_block{blockid}')(relu_4)\n",
    "    conv1d_5 = Conv1D(filters=filters/5,kernel_size= 64 ,padding='same',name=f'conv1d_5_block{blockid}')(relu_5)\n",
    "    conv1d_6 = Conv1D(filters=filters/6,kernel_size=128 ,padding='same',name=f'conv1d_6_block{blockid}')(relu_6)\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name=f'batch_norm_2_1_block{blockid}')(conv1d_1)\n",
    "    batch_norm_2 = BatchNormalization(name=f'batch_norm_2_2_block{blockid}')(conv1d_2)\n",
    "    batch_norm_3 = BatchNormalization(name=f'batch_norm_2_3_block{blockid}')(conv1d_3)\n",
    "    batch_norm_4 = BatchNormalization(name=f'batch_norm_2_4_block{blockid}')(conv1d_4)\n",
    "    batch_norm_5 = BatchNormalization(name=f'batch_norm_2_5_block{blockid}')(conv1d_5)\n",
    "    batch_norm_6 = BatchNormalization(name=f'batch_norm_2_6_block{blockid}')(conv1d_6)\n",
    "\n",
    "    relu_1 = ReLU(name=f'relu_2_1_block{blockid}')(batch_norm_1)\n",
    "    relu_2 = ReLU(name=f'relu_2_2_block{blockid}')(batch_norm_2)\n",
    "    relu_3 = ReLU(name=f'relu_2_3_block{blockid}')(batch_norm_3)\n",
    "    relu_4 = ReLU(name=f'relu_2_4_block{blockid}')(batch_norm_4)\n",
    "    relu_5 = ReLU(name=f'relu_2_5_block{blockid}')(batch_norm_5)\n",
    "    relu_6 = ReLU(name=f'relu_2_6_block{blockid}')(batch_norm_6)\n",
    "\n",
    "    return relu_1,relu_2,relu_3,relu_4,relu_5,relu_6\n",
    "\n",
    "def two_stride_conv_block(relu_1,relu_2,relu_3,relu_4,relu_5,relu_6,filters:int,blockid:int):\n",
    "\n",
    "    depthwiseconv1d_1 = DepthwiseConv1D(kernel_size=  4 ,padding='same',strides= 2,name=f'depthconv1d_1_block{blockid}')(relu_1)\n",
    "    depthwiseconv1d_2 = DepthwiseConv1D(kernel_size=  8 ,padding='same',strides= 2,name=f'depthconv1d_2_block{blockid}')(relu_2)\n",
    "    depthwiseconv1d_3 = DepthwiseConv1D(kernel_size= 16 ,padding='same',strides= 2,name=f'depthconv1d_3_block{blockid}')(relu_3)\n",
    "    depthwiseconv1d_4 = DepthwiseConv1D(kernel_size= 32 ,padding='same',strides= 2,name=f'depthconv1d_4_block{blockid}')(relu_4)\n",
    "    depthwiseconv1d_5 = DepthwiseConv1D(kernel_size= 64 ,padding='same',strides= 2,name=f'depthconv1d_5_block{blockid}')(relu_5)\n",
    "    depthwiseconv1d_6 = DepthwiseConv1D(kernel_size=128 ,padding='same',strides= 2,name=f'depthconv1d_6_block{blockid}')(relu_6)\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name=f'batch_norm_1_1_block{blockid}')(depthwiseconv1d_1)\n",
    "    batch_norm_2 = BatchNormalization(name=f'batch_norm_1_2_block{blockid}')(depthwiseconv1d_2)\n",
    "    batch_norm_3 = BatchNormalization(name=f'batch_norm_1_3_block{blockid}')(depthwiseconv1d_3)\n",
    "    batch_norm_4 = BatchNormalization(name=f'batch_norm_1_4_block{blockid}')(depthwiseconv1d_4)\n",
    "    batch_norm_5 = BatchNormalization(name=f'batch_norm_1_5_block{blockid}')(depthwiseconv1d_5)\n",
    "    batch_norm_6 = BatchNormalization(name=f'batch_norm_1_6_block{blockid}')(depthwiseconv1d_6)\n",
    "\n",
    "    relu_1 = ReLU(name=f'relu_1_1_block{blockid}')(batch_norm_1)\n",
    "    relu_2 = ReLU(name=f'relu_1_2_block{blockid}')(batch_norm_2)\n",
    "    relu_3 = ReLU(name=f'relu_1_3_block{blockid}')(batch_norm_3)\n",
    "    relu_4 = ReLU(name=f'relu_1_4_block{blockid}')(batch_norm_4)\n",
    "    relu_5 = ReLU(name=f'relu_1_5_block{blockid}')(batch_norm_5)\n",
    "    relu_6 = ReLU(name=f'relu_1_6_block{blockid}')(batch_norm_6)\n",
    "\n",
    "    conv1d_1 = Conv1D(filters=filters/1,kernel_size=  4 ,padding='same',name=f'conv1d_1_block{blockid}')(relu_1)\n",
    "    conv1d_2 = Conv1D(filters=filters/2,kernel_size=  8 ,padding='same',name=f'conv1d_2_block{blockid}')(relu_2)\n",
    "    conv1d_3 = Conv1D(filters=filters/3,kernel_size= 16 ,padding='same',name=f'conv1d_3_block{blockid}')(relu_3)\n",
    "    conv1d_4 = Conv1D(filters=filters/4,kernel_size= 32 ,padding='same',name=f'conv1d_4_block{blockid}')(relu_4)\n",
    "    conv1d_5 = Conv1D(filters=filters/5,kernel_size= 64 ,padding='same',name=f'conv1d_5_block{blockid}')(relu_5)\n",
    "    conv1d_6 = Conv1D(filters=filters/6,kernel_size=128 ,padding='same',name=f'conv1d_6_block{blockid}')(relu_6)\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name=f'batch_norm_2_1_block{blockid}')(conv1d_1)\n",
    "    batch_norm_2 = BatchNormalization(name=f'batch_norm_2_2_block{blockid}')(conv1d_2)\n",
    "    batch_norm_3 = BatchNormalization(name=f'batch_norm_2_3_block{blockid}')(conv1d_3)\n",
    "    batch_norm_4 = BatchNormalization(name=f'batch_norm_2_4_block{blockid}')(conv1d_4)\n",
    "    batch_norm_5 = BatchNormalization(name=f'batch_norm_2_5_block{blockid}')(conv1d_5)\n",
    "    batch_norm_6 = BatchNormalization(name=f'batch_norm_2_6_block{blockid}')(conv1d_6)\n",
    "\n",
    "    relu_1 = ReLU(name=f'relu_2_1_block{blockid}')(batch_norm_1)\n",
    "    relu_2 = ReLU(name=f'relu_2_2_block{blockid}')(batch_norm_2)\n",
    "    relu_3 = ReLU(name=f'relu_2_3_block{blockid}')(batch_norm_3)\n",
    "    relu_4 = ReLU(name=f'relu_2_4_block{blockid}')(batch_norm_4)\n",
    "    relu_5 = ReLU(name=f'relu_2_5_block{blockid}')(batch_norm_5)\n",
    "    relu_6 = ReLU(name=f'relu_2_6_block{blockid}')(batch_norm_6)\n",
    "\n",
    "    return relu_1,relu_2,relu_3,relu_4,relu_5,relu_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CANDEL MODEL\n",
    "\n",
    "input_layer = Input(shape=(256,3))\n",
    "reshaped_candels = Reshape(target_shape=(256,3,1),name=\"reshape_0\")(input_layer)\n",
    "#Converting candels into 1d arrays\n",
    "\n",
    "reshape_1,reshape_2,reshape_3,reshape_4,reshape_5,reshape_6 = convert_candel_to_1d_array(\n",
    "                                                                                            candels_reshape=reshaped_candels,\n",
    "                                                                                            filters=32\n",
    "                                                                                        )\n",
    "\n",
    "#Block1 with strides = 1, shape sizes of: 127 , 125 , 121 , 113 , 97 , 65 \n",
    "\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = one_stride_conv_block(\n",
    "                                                                    reshape_1,\n",
    "                                                                    reshape_2,\n",
    "                                                                    reshape_3,\n",
    "                                                                    reshape_4,\n",
    "                                                                    reshape_5,\n",
    "                                                                    reshape_6,\n",
    "                                                                    filters=32,\n",
    "                                                                    blockid=1\n",
    "                                                                )\n",
    "\n",
    "\n",
    "#Block 2 with stride = 2, shape sizes of: 64 , 63 , 61 , 57 , 49 , 33\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=64,\n",
    "                                                                    blockid=2\n",
    "                                                                )\n",
    "\n",
    "#Block 3 with stride = 2, shape sizes of: 32 , 32, 31, 29, 25 , 17\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=128,\n",
    "                                                                    blockid=3\n",
    "                                                                )\n",
    "\n",
    "#Block 4 with stride = 2, shape sizes of: 16, 16, 16, 15, 13,  9\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=256,\n",
    "                                                                    blockid=4\n",
    "                                                                )\n",
    "\n",
    "#Block 5 with stride = 2, shape sizes of: 8, 8, 8, 8, 7, 5\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=256,\n",
    "                                                                    blockid=5\n",
    "                                                                )\n",
    "\n",
    "\n",
    "global_avg_pool_1 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_1\")(relu_1)\n",
    "global_avg_pool_2 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_2\")(relu_2)\n",
    "global_avg_pool_3 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_3\")(relu_3)\n",
    "global_avg_pool_4 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_4\")(relu_4)\n",
    "global_avg_pool_5 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_5\")(relu_5)\n",
    "global_avg_pool_6 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_6\")(relu_6)\n",
    "\n",
    "\n",
    "candel_model = keras.Model(\n",
    "                        inputs = input_layer,\n",
    "\n",
    "                        outputs = [\n",
    "                                    global_avg_pool_1,\n",
    "                                    global_avg_pool_2,\n",
    "                                    global_avg_pool_3,\n",
    "                                    global_avg_pool_4,\n",
    "                                    global_avg_pool_5,\n",
    "                                    global_avg_pool_6\n",
    "                                ]\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDICATOR MODEL\n",
    "\n",
    "input_layer = Input(shape=(256,10))\n",
    "\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = one_stride_conv_block(\n",
    "                                                                    input_layer,\n",
    "                                                                    input_layer,\n",
    "                                                                    input_layer,\n",
    "                                                                    input_layer,\n",
    "                                                                    input_layer,\n",
    "                                                                    input_layer,\n",
    "                                                                    filters=32,\n",
    "                                                                    blockid=10\n",
    "                                                                )\n",
    "\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = one_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=32,\n",
    "                                                                    blockid=11\n",
    "                                                                )\n",
    "\n",
    "\n",
    "#Block 2 with stride = 2, shape sizes of: 64 , 63 , 61 , 57 , 49 , 33\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=64,\n",
    "                                                                    blockid=12\n",
    "                                                                )\n",
    "\n",
    "#Block 3 with stride = 2, shape sizes of: 32 , 32, 31, 29, 25 , 17\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=128,\n",
    "                                                                    blockid=13\n",
    "                                                                )\n",
    "\n",
    "#Block 4 with stride = 2, shape sizes of: 16, 16, 16, 15, 13,  9\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=256,\n",
    "                                                                    blockid=14\n",
    "                                                                )\n",
    "\n",
    "#Block 5 with stride = 2, shape sizes of: 8, 8, 8, 8, 7, 5\n",
    "relu_1,relu_2,relu_3,relu_4,relu_5,relu_6 = two_stride_conv_block(\n",
    "                                                                    relu_1,\n",
    "                                                                    relu_2,\n",
    "                                                                    relu_3,\n",
    "                                                                    relu_4,\n",
    "                                                                    relu_5,\n",
    "                                                                    relu_6,\n",
    "                                                                    filters=256,\n",
    "                                                                    blockid=15\n",
    "                                                                )\n",
    "\n",
    "\n",
    "global_avg_pool_1 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_1\")(relu_1)\n",
    "global_avg_pool_2 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_2\")(relu_2)\n",
    "global_avg_pool_3 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_3\")(relu_3)\n",
    "global_avg_pool_4 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_4\")(relu_4)\n",
    "global_avg_pool_5 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_5\")(relu_5)\n",
    "global_avg_pool_6 = keras.layers.GlobalAveragePooling1D(name=\"global_avg_pool_indicator_6\")(relu_6)\n",
    "\n",
    "\n",
    "indicator_model = keras.Model(\n",
    "                        inputs = input_layer,\n",
    "                        outputs = [\n",
    "                                    global_avg_pool_1,\n",
    "                                    global_avg_pool_2,\n",
    "                                    global_avg_pool_3,\n",
    "                                    global_avg_pool_4,\n",
    "                                    global_avg_pool_5,\n",
    "                                    global_avg_pool_6\n",
    "                                ]\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "input_layer = Input(shape=(256,13))\n",
    "candels = Lambda(lambda x: x[:,:,-3:])(input_layer)\n",
    "indicators = Lambda(lambda x: x[:,:,:-3])(input_layer)\n",
    "\n",
    "gap_candel_1,gap_candel_2,gap_candel_3,gap_candel_4,gap_candel_5,gap_candel_6=candel_model(candels)\n",
    "gap_indicator_1,gap_indicator_2,gap_indicator_3,gap_indicator_4,gap_indicator_5,gap_indicator_6 = indicator_model(indicators)\n",
    "\n",
    "\n",
    "concatinate = Concatenate()([   gap_candel_1,\n",
    "                                gap_candel_2,\n",
    "                                gap_candel_3,\n",
    "                                gap_candel_4,\n",
    "                                gap_candel_5,\n",
    "                                gap_candel_6,\n",
    "                                gap_indicator_1,\n",
    "                                gap_indicator_2,\n",
    "                                gap_indicator_3,\n",
    "                                gap_indicator_4,\n",
    "                                gap_indicator_5,\n",
    "                                gap_indicator_6]\n",
    ")\n",
    "\n",
    "dense01 = Dense(1024 , activation='relu')(concatinate)\n",
    "output = Dense(2 ,activation='softmax')(dense01)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs = input_layer,\n",
    "    outputs = output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator_model.summary()\n",
    "# tf.keras.utils.plot_model(model ,'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: saving model to tmp\\cp-0001.ckpt\n",
      "1277/1277 - 428s - loss: 0.6883 - binary_accuracy: 0.5520 - val_loss: 1.0764 - val_binary_accuracy: 0.4844 - 428s/epoch - 335ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: saving model to tmp\\cp-0002.ckpt\n",
      "1277/1277 - 373s - loss: 0.6351 - binary_accuracy: 0.6351 - val_loss: 0.7359 - val_binary_accuracy: 0.5702 - 373s/epoch - 292ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: saving model to tmp\\cp-0003.ckpt\n",
      "1277/1277 - 362s - loss: 0.5268 - binary_accuracy: 0.7378 - val_loss: 0.6722 - val_binary_accuracy: 0.6727 - 362s/epoch - 283ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: saving model to tmp\\cp-0004.ckpt\n",
      "1277/1277 - 362s - loss: 0.4376 - binary_accuracy: 0.7974 - val_loss: 0.6145 - val_binary_accuracy: 0.7010 - 362s/epoch - 284ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: saving model to tmp\\cp-0005.ckpt\n",
      "1277/1277 - 360s - loss: 0.3775 - binary_accuracy: 0.8334 - val_loss: 0.5672 - val_binary_accuracy: 0.7204 - 360s/epoch - 282ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: saving model to tmp\\cp-0006.ckpt\n",
      "1277/1277 - 363s - loss: 0.3381 - binary_accuracy: 0.8539 - val_loss: 0.5600 - val_binary_accuracy: 0.7419 - 363s/epoch - 284ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: saving model to tmp\\cp-0007.ckpt\n",
      "1277/1277 - 360s - loss: 0.3053 - binary_accuracy: 0.8698 - val_loss: 0.7969 - val_binary_accuracy: 0.6665 - 360s/epoch - 282ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: saving model to tmp\\cp-0008.ckpt\n",
      "1277/1277 - 368s - loss: 0.2780 - binary_accuracy: 0.8828 - val_loss: 0.6318 - val_binary_accuracy: 0.7404 - 368s/epoch - 288ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: saving model to tmp\\cp-0009.ckpt\n",
      "1277/1277 - 372s - loss: 0.2528 - binary_accuracy: 0.8953 - val_loss: 0.7954 - val_binary_accuracy: 0.7142 - 372s/epoch - 292ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: saving model to tmp\\cp-0010.ckpt\n",
      "1277/1277 - 373s - loss: 0.2322 - binary_accuracy: 0.9039 - val_loss: 2.8052 - val_binary_accuracy: 0.5352 - 373s/epoch - 292ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a227064c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#name of the model \n",
    "name = \"6_(functional)_(22_04_02)\"\n",
    "\n",
    "compile_and_fit(model=model , modelname=name,\n",
    "                data = data,input_window=input_window,\n",
    "                output_window=output_window,modelcheckpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'models/{name}/model.h5')\n",
    "\n",
    "\n",
    "# model = keras.models.load_model(f'models/{name}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test , output_test = data.make_windows(data.input_test_dataset,data.output_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=model.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = predicted.tolist()\n",
    "output_list = output_test.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(len(predicted_list)):\n",
    "    if(predicted_list[i][0]>0.9 or predicted_list[i][1]>0.9):\n",
    "        counter+=1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "pred= []\n",
    "\n",
    "for index in range(len(predicted_list)):\n",
    "    \n",
    "    if(predicted_list[index][0]>0.9):\n",
    "        pred.append(predicted_list[index])\n",
    "        real.append(output_list[index])\n",
    "    \n",
    "    elif(predicted_list[index][1]>0.9):\n",
    "        pred.append(predicted_list[index])\n",
    "        real.append(output_list[index])\n",
    "\n",
    "len(real) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "m.update_state(y_true = real,y_pred = pred)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = np.ones((3,3),dtype=np.int8) + 0\n",
    "# a2 = np.ones((3,3),dtype=np.int8) + 1\n",
    "# a3 = np.ones((3,3),dtype=np.int8) + 2\n",
    "# a4 = np.ones((3,3),dtype=np.int8) + 3\n",
    "\n",
    "# a11 = np.ones((3,3),dtype=np.int8) * 10\n",
    "# a12 = np.ones((3,3),dtype=np.int8) * 20\n",
    "# a13 = np.ones((3,3),dtype=np.int8) * 30\n",
    "# a14 = np.ones((3,3),dtype=np.int8) * 40\n",
    "\n",
    "# inputs = np.array([a1,a2,a3,a4])\n",
    "# outputs = np.array([a11,a12,a13,a14])\n",
    "# a , b = sklearn.utils.shuffle(inputs,outputs)\n",
    "# print(f'{[a123 for a123 in a]},{[b123 for b123 in b]}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b0900fbe6dcf68ba8657d6a73781eea6c8e04d861aa42c88ba789e96c4944de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
